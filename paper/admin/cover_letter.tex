\documentclass[10pt,stdletter,orderfromtodate,sigleft]{newlfm}
\usepackage{blindtext, xfrac, animate, hyperref, pxfonts}
\usepackage[subtle]{savetrees}

\setlength{\voffset}{0in}

\newlfmP{dateskipbefore=0pt}
\newlfmP{sigsize=10pt}
\newlfmP{sigskipbefore=10pt}
 
\newlfmP{Headlinewd=0pt,Footlinewd=0pt}
 
\namefrom{Jeremy R. Manning, Ph.D.}
\addrfrom{}
 
\addrto{}
\dateset{}
 
\greetto{\vspace{-1in}To the editors of \textit{Scientific Reports}:}
 
\closeline{Sincerely,}

\begin{document}
\begin{newlfm}

We have enclosed our manuscript entitled \textit{Category-based and
location-based volitional covert attention affect memory at different
timescales}, which we wish to submit for publication as an Article in
\textit{Scientific Reports}.

Our manuscript reports our findings from an experiment designed to study how
different aspects of covert attention affect memory. Our experiment comprised
two conditions that asked participants to \textit{sustain} or \textit{vary} the
focus of their covert attention, respectively, with respect to each of a series
of stimuli. Each stimulus comprised a pair of images (one on the left of the
screen and the other on the right), and each image comprised an equal blend of
a photograph of a face and a photograph of a scene. During each stimulus
presentation, we instructed participants to focus in on either the face or
scene component of either the left or right image \textit{without moving their
eyes}. (This is quite difficult to do naively, so we trained participants to do
this quickly and at will using a practice session prior to the main
experiment.) We used eye-tracking data to focus in on trials where participants
were specifically varying their focus of \textit{covert} attention (i.e., with
no change in where they were looking), as opposed to simply moving their eyes
to look at the to-be-attended images. We then administered recognition memory
tests that asked participants to rate the ``familiarity'' of attended,
unattended, and novel images.

As we had expected, participants in both conditions remembered stimuli they
attended to better than unattended stimuli, and they rated both attended and
unattended stimuli they had seen as more ``familiar'' than novel images, during
a recognition memory test. In addition, we also found some interesting
(unexpected) differences across the two experimental conditions pertaining to how
participants remembered ``partially attended'' stimuli. When participants
varied the focus of their attention more often, their memory was more
influenced by location-based attention (regardless of which image category they
were attending). In contrast, when participants sustained the focus of their
attention over longer intervals, their memory was more influenced by
category-based attention (regardless of which location they were attending).
Our findings highlight a dissociation between how location-based and
category-based attention affect memory encoding (and subsequent recognition).
Location-based attention appears to affect memory encoding on relatively short
timescales by enhancing encoding for stimuli at the attended location. On the
other hand, category-based attention appears to affect memory encoding on
longer timescales by \textit{suppressing} stimuli from the unattended category.

The impact of attention on memory is of central importance to a wide variety of
scientifically and socially current issues. For example, the interplay between
attention and memory affects how we remember our ongoing experiences in
everyday life, how students learn in the classroom (and which teaching
strategies might be most effective), how (or whether) we recognize a face we
notice in a crowd and happen to encounter later, and so on. These issues also
relate to attention disorders (e.g., ADD, ADHD) and memory disorders (e.g.,
age-related memory impairment, Alzheimer's, etc.). Discovering the processes
that guide how our experiences are encoded into memories can lead to new tools
for identifying when those processes are not operating within their expected
parameters.

In the spirit of reproducibility and transparency, we have also published and
documented all of our experimental stimuli, code, and data, along with the code
used to generate the figures and analyses we report in our manuscript. These
may be found at
\href{https://github.com/ContextLab/attention-memory-task}{\texttt{https://github.com/ContextLab/attention-memory-task}}.

Thank you for considering our manuscript for publication in \textit{Scientific Reports}.

\end{newlfm}
\end{document}


