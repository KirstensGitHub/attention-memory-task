{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: davos in /Users/kirstenziman/anaconda3/lib/python3.11/site-packages (0.2.3)\n",
      "Requirement already satisfied: packaging in /Users/kirstenziman/anaconda3/lib/python3.11/site-packages (from davos) (23.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install davos\n",
    "#import davos\n",
    "\n",
    "#davos.config.suppress_stdout = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "smuggle numpy as np               # pip: numpy==1.24.2\n",
    "smuggle matplotlib.pyplot as plt  # pip: matplotlib==3.7.0\n",
    "from matplotlib.patches smuggle Rectangle\n",
    "from matplotlib.collections smuggle PatchCollection\n",
    "smuggle pandas as pd              # pip: pandas==1.5.3\n",
    "smuggle seaborn as sns            # pip: seaborn==0.12.2\n",
    "from scipy.stats smuggle ttest_rel, ttest_ind, pearsonr      # pip: scipy==1.10.1\n",
    "from tqdm smuggle tqdm            # pip: tqdm==4.64.1\n",
    "\n",
    "smuggle requests                  # pip: requests==2.28.2\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "from helpers import load_data, figdir, attention_colors, recency, plot_colorbar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download dataset, set up paths, and load behavioral and gaze data into the current workspace:\n",
    "  - sustained, variable: behavioral data with labels added and all trials removed where participants looked at the attended image (used in most analyses)\n",
    "  - sustained_gaze, variable_gaze: eyetracker data (used to summarize/plot gaze data)\n",
    "  - sustained_unfiltered, variable_unfiltered: behavioral data with no trials removed (used to summarize which trials were filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███| 30/30 [00:04<00:00,  7.34it/s]\n",
      "100%|███| 23/23 [00:03<00:00,  7.17it/s]\n",
      "100%|███| 30/30 [00:40<00:00,  1.34s/it]\n",
      "100%|███| 23/23 [00:42<00:00,  1.84s/it]\n",
      "100%|███| 30/30 [00:05<00:00,  5.98it/s]\n",
      "100%|███| 23/23 [00:03<00:00,  6.49it/s]\n",
      "2400it [00:05, 476.10it/s]\n",
      "1840it [00:03, 524.66it/s]\n",
      "2532it [00:04, 557.72it/s]"
     ]
    }
   ],
   "source": [
    "sustained_gaze, variable_gaze, sustained_unfiltered, variable_unfiltered = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sustained_gaze_orig = sustained_gaze.copy\n",
    "sustained_gaze_new  = sustained_gaze.drop_duplicates()\n",
    "\n",
    "sustained_gaze_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_unfiltered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_gaze_orig = variable_gaze.copy\n",
    "variable_gaze_new  = variable_gaze.drop_duplicates()\n",
    "\n",
    "variable_gaze_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, let's filter the behavioral data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(df, intersection='Attended intersection', min_hertz=20):\n",
    "    '''\n",
    "    this function returns filtered behavioral data\n",
    "    \n",
    "    it removes :\n",
    "    - presentation trials where the intersection criteria are not met\n",
    "    - presentation trials where the minimum amount of gaze data is not recorded\n",
    "    - memory trials showing images for which intersection criteria were not met \n",
    "    - memory trials showing images for which the minimum amount of gaze data was not recorded \n",
    "    \n",
    "    pres options: 'Attended intersection', 'Intersection detected'\n",
    "    min_hertz: int or float\n",
    "    of any value\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    df_list = []\n",
    "    \n",
    "    # filter presentation trials based on criteria\n",
    "    filtered_data_pres = df[  (df['Trial Type'] == 'Presentation') \n",
    "                            & (df[intersection] == False)\n",
    "                            & (df['first_second' ] >= min_hertz)\n",
    "                            & (df['second_second'] >= min_hertz)\n",
    "                            & (df['third_second' ] >= min_hertz)\n",
    "                           ]\n",
    "    # note: we check the minimum number of gaze datapoints in each second of the 3s presentation trial\n",
    "    \n",
    "    # append filtered presentation data to df_list\n",
    "    df_list.append(filtered_data_pres)\n",
    "    \n",
    "    # for each subject\n",
    "    for s in filtered_data_pres['Subject'].unique():\n",
    "        \n",
    "        # get the data for this subject\n",
    "        sub_df      = df[ df['Subject'] == s ]\n",
    "        sub_pres_df = filtered_data_pres[filtered_data_pres['Subject'] == s]\n",
    "    \n",
    "        # get the images from the trials where this subject had good data\n",
    "        filtered_pres_images = list(sub_pres_df['Cued Place']) + list(sub_pres_df['Cued Face']) + list(sub_pres_df['Uncued Place']) + list(sub_pres_df['Uncued Face'])\n",
    "\n",
    "        \n",
    "        # get the corresponding memory trials\n",
    "        filtered_data_mem_1    = sub_df[ (sub_df['Trial Type'] == 'Memory')\n",
    "                                       & (sub_df['Memory Image'].isin(filtered_pres_images))\n",
    "                                       ]\n",
    "        \n",
    "        # also get memory trials with novel images\n",
    "        filtered_data_mem_2    = sub_df[ (sub_df['Trial Type'] == 'Memory')\n",
    "                                       & (sub_df['Attention']  == 'Novel')\n",
    "                                       ]\n",
    "        \n",
    "        # append filtered memory data for this subject to the df_list \n",
    "        df_list.append( filtered_data_mem_1 )\n",
    "        df_list.append( filtered_data_mem_2 )\n",
    "        \n",
    "    # concatenate all filtered dataframes\n",
    "    full_filtered = pd.concat(df_list)\n",
    "    \n",
    "    \n",
    "    return( full_filtered )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sustained = filter_data(sustained_unfiltered)\n",
    "variable  = filter_data(variable_unfiltered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sustained.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sustained_unfiltered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sustained_unfiltered['Subject'].unique()))\n",
    "print(len(sustained['Subject'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(12000-8282) / 12000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_unfiltered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(9200-5863) / 9200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(variable_unfiltered['Subject'].unique()))\n",
    "print(len(variable['Subject'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure S1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(1, 2, figsize=(10, 3), sharex=True, sharey=True)\n",
    "\n",
    "# sns.histplot(sustained_gaze, x='x', y='y', cbar=False, stat='probability', cmap='gray_r', bins=(120, 78), ax=ax[0], vmin=0, vmax=0.03)\n",
    "# ax[0].text(59.8 / 2, 33.6 / 2, '+', ha='center', va='center', fontsize=10, color='red', fontweight='bold')\n",
    "\n",
    "# # KZ edited this line\n",
    "# # im_len = 6.7 * (52.96 / 59.8)\n",
    "# im_len = 6.7 * (59.8 / 52.96)\n",
    "\n",
    "# # this line intends to give us the length of an image stimulus in centimeters (instead of DVA)\n",
    "# # so what (I think?) we want is:\n",
    "# # im_len_in_cm  =  6.7 DVA  *  ( 59.8 cm  /  52.96 DVA )\n",
    "\n",
    "# # ^^ I could definitely just be looking at this through tired eyes so it would be great if you could confirm!\n",
    "\n",
    "# y = (33.6 - im_len) / 2\n",
    "# x1 = (59.8 / 2) - 4.5 - im_len\n",
    "# x2 = (59.8 / 2) + 4.5\n",
    "\n",
    "# images = [Rectangle((x1, y), im_len, im_len, fill=False, color='red', lw=1),\n",
    "#           Rectangle((x2, y), im_len, im_len, fill=False, color='red', lw=1)]\n",
    "# pc = PatchCollection(images, match_original=True)\n",
    "# ax[0].add_collection(pc)\n",
    "\n",
    "# ax[0].set_xlabel('x (cm)', fontsize=12)\n",
    "# ax[0].set_ylabel('y (cm)', fontsize=12)\n",
    "# ax[0].set_title('Sustained', fontsize=12)\n",
    "\n",
    "# sns.histplot(variable_gaze, x='x', y='y', cbar=False, stat='probability', cmap='gray_r', bins=(120, 78), ax=ax[1], vmin=0, vmax=0.03)\n",
    "# ax[1].text(59.8 / 2, 33.6 / 2, '+', ha='center', va='center', fontsize=10, color='red', fontweight='bold')\n",
    "# pc = PatchCollection(images, match_original=True)\n",
    "# ax[1].add_collection(pc)\n",
    "\n",
    "# ax[1].set_xlim(0, 59.8)\n",
    "# ax[1].set_ylim(0, 33.6)\n",
    "# ax[1].set_xlabel('x (cm)', fontsize=12)\n",
    "# ax[1].set_ylabel('y (cm)', fontsize=12)\n",
    "# ax[1].set_title('Variable', fontsize=12)\n",
    "\n",
    "# plt.subplots_adjust(wspace=0.05)\n",
    "# fig.savefig(os.path.join(figdir, 'gaze_distribution.pdf'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 3), sharex=True, sharey=True)\n",
    "\n",
    "# kz edit: used the gaze data with duplicates dropped\n",
    "#sns.histplot(sustained_gaze, x='x', y='y', cbar=False, stat='probability', cmap='gray_r', bins=(120, 78), ax=ax[0], vmin=0, vmax=0.03)\n",
    "sns.histplot(sustained_gaze_new, x='x', y='y', cbar=False, stat='probability', cmap='gray_r', bins=(120, 78), ax=ax[0], vmin=0, vmax=0.03)\n",
    "ax[0].text(59.8 / 2, 33.6 / 2, '+', ha='center', va='center', fontsize=10, color='red', fontweight='bold')\n",
    "\n",
    "\n",
    "im_len = 6.7 * ( 59.8 / 52.96 ) # <- kz added this line\n",
    "# im_len = 6.7 * (52.96 / 59.8) # <- kz commented this line\n",
    "\n",
    "# KZ NOTE: im_len_in_cm = 6.7 DVA * ( 59.8 cm / 52.96 DVA )\n",
    "# KZ NOTE: this conversion factor has also been change in the helpers.py\n",
    "\n",
    "y = (33.6 - im_len) / 2\n",
    "x1 = (59.8 / 2) - 4.5 - im_len\n",
    "x2 = (59.8 / 2) + 4.5\n",
    "\n",
    "\n",
    "# kz added these lines: boundary one cm around the images ##############################\n",
    "im_len_a = 6.7 * (59.8 / 52.96) + 2\n",
    "y_a = (33.6 - im_len_a) / 2\n",
    "x1_a = (59.8 / 2) - 3.5 - im_len_a\n",
    "x2_a = (59.8 / 2) + 3.5\n",
    "########################################################################################\n",
    "\n",
    "images = [Rectangle((x1, y), im_len, im_len, fill=False, color='red', lw=1),\n",
    "          Rectangle((x2, y), im_len, im_len, fill=False, color='red', lw=1)]\n",
    "pc = PatchCollection(images, match_original=True)\n",
    "ax[0].add_collection(pc)\n",
    "\n",
    "\n",
    "# kz added these lines - draw bounding box around the images ##########################\n",
    "bounding = [Rectangle((x1_a, y_a), im_len_a, im_len_a, fill=False, color='gray', lw=1),\n",
    "          Rectangle((x2_a, y_a), im_len_a, im_len_a, fill=False, color='gray', lw=1)]\n",
    "bounds = PatchCollection(bounding, match_original=True)\n",
    "ax[0].add_collection(bounds)\n",
    "########################################################################################\n",
    "\n",
    "ax[0].set_xlabel('x (cm)', fontsize=12)\n",
    "ax[0].set_ylabel('y (cm)', fontsize=12)\n",
    "ax[0].set_title('Sustained', fontsize=12)\n",
    "\n",
    "# kz edit: used the gaze data with duplicates dropped\n",
    "# sns.histplot(variable_gaze, x='x', y='y', cbar=False, stat='probability', cmap='gray_r', bins=(120, 78), ax=ax[1], vmin=0, vmax=0.03)\n",
    "sns.histplot(variable_gaze_new, x='x', y='y', cbar=False, stat='probability', cmap='gray_r', bins=(120, 78), ax=ax[1], vmin=0, vmax=0.03)\n",
    "ax[1].text(59.8 / 2, 33.6 / 2, '+', ha='center', va='center', fontsize=10, color='red', fontweight='bold')\n",
    "pc = PatchCollection(images, match_original=True)\n",
    "bounds = PatchCollection(bounding, match_original=True) # <- kz added this line\n",
    "ax[1].add_collection(pc)\n",
    "ax[1].add_collection(bounds) # <- kz added this line\n",
    "\n",
    "ax[1].set_xlim(0, 59.8)\n",
    "ax[1].set_ylim(0, 33.6)\n",
    "ax[1].set_xlabel('x (cm)', fontsize=12)\n",
    "ax[1].set_ylabel('y (cm)', fontsize=12)\n",
    "ax[1].set_title('Variable', fontsize=12)\n",
    "\n",
    "plt.subplots_adjust(wspace=0.05)\n",
    "fig.savefig(os.path.join(figdir, 'gaze_distribution.pdf'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_colorbar('gray_r', 'gray_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confirm fast reaction time to prode on attended versus unattended side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_intersections(df, category='Attended intersection'):\n",
    "    df = df.query('`Trial Type` == \"Presentation\"').fillna(False)\n",
    "    return df[['Subject', 'Run', category]].groupby(['Subject', 'Run']).mean().reset_index().rename({category: 'Intersection'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection_type = 'Attended intersection'\n",
    "\n",
    "def plot_intersections(intersection_type, fname=None):\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(8, 6), sharey=True)\n",
    "    # sustained\n",
    "    df = summarize_intersections(sustained_unfiltered, category=intersection_type)\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        sns.barplot(data=df, x='Run', y='Intersection', ax=ax[0, 0], color='gray')\n",
    "    ax[0, 0].set_title('Sustained', fontsize=12);\n",
    "    ax[0, 0].set_ylabel('Proportion of trials', fontsize=12);\n",
    "    ax[0, 0].set_xlabel('Trial block', fontsize=12);\n",
    "    sns.despine(ax=ax[0, 0], top=True, right=True)\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        sns.barplot(data=df, x='Subject', y='Intersection', ax=ax[1, 0], color='gray')\n",
    "    ax[1, 0].set_ylabel('Proportion of trials', fontsize=12);\n",
    "    ax[1, 0].set_xlabel('Participant', fontsize=12);\n",
    "    ax[1, 0].set_xticklabels([i if i % 5 == 0 else '' for i in range(len(df['Subject'].unique()))]);\n",
    "    sns.despine(ax=ax[1, 0], top=True, right=True)\n",
    "\n",
    "    # variable\n",
    "    df = summarize_intersections(variable_unfiltered, category=intersection_type)\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        sns.barplot(data=df, x='Run', y='Intersection', ax=ax[0, 1], color='gray')\n",
    "    ax[0, 1].set_title('Variable', fontsize=12);\n",
    "    ax[0, 1].set_xlabel('Trial block', fontsize=12);\n",
    "    ax[0, 1].set_ylabel('');\n",
    "    sns.despine(ax=ax[0, 1], top=True, right=True)\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        sns.barplot(data=df, x='Subject', y='Intersection', ax=ax[1, 1], color='gray')\n",
    "    ax[1, 1].set_ylabel('');\n",
    "    ax[1, 1].set_xlabel('Participant', fontsize=12);\n",
    "    ax[1, 1].set_xticklabels([i if i % 5 == 0 else '' for i in range(len(df['Subject'].unique()))]);\n",
    "    sns.despine(ax=ax[1, 1], top=True, right=True)\n",
    "\n",
    "    ax[0, 0].set_ylim(0, 1.1)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if fname is not None:\n",
    "        fig.savefig(os.path.join(figdir, fname + '.pdf'), bbox_inches='tight')\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure S2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_intersections(intersection_type, intersection_type.replace(' ', '_'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(df, fname=None, palette=attention_colors, ylim=[1.6, 3], **kwargs):\n",
    "    df = df.copy()\n",
    "\n",
    "    fig = plt.figure(figsize=(4, 3))\n",
    "    ax = plt.gca()\n",
    "\n",
    "    order = ['Attended', 'Attended category', 'Attended location', 'Unattended', 'Novel']\n",
    "\n",
    "    id_vars = ['Subject', 'Attention', 'Run']\n",
    "    if 'hue' in kwargs:\n",
    "        id_vars.append(kwargs['hue'])\n",
    "    \n",
    "    sns.barplot(data=df.query('`Trial Type` == \"Memory\"')[[*id_vars, 'Familiarity Rating']].groupby(id_vars).mean().reset_index(), \n",
    "                   x='Attention', y='Familiarity Rating', order=['Attended', 'Attended category', 'Attended location', 'Unattended', 'Novel'],  palette=palette, **kwargs);\n",
    "    ax.set_xlabel('Attention level', fontsize=12);\n",
    "    ax.set_ylabel('Familiarity rating', fontsize=12);\n",
    "    ax.set_xticklabels(['' for _ in range(len(ax.get_xticklabels()))]);\n",
    "    ax.set_ylim(ylim)\n",
    "\n",
    "    if 'hue' in kwargs:                \n",
    "        n = len(df[kwargs['hue']].unique()) - 1  # not sure why this correction is needed...\n",
    "        alphas = np.linspace(1, 0, n + 1)[:-1]\n",
    "\n",
    "        for i in range(n):\n",
    "            for j, bar in enumerate(ax.containers[i]):\n",
    "                bar.set_color(palette[order[j]])\n",
    "                bar.set_alpha(alphas[i])\n",
    "\n",
    "    sns.despine(top=True, right=True)\n",
    "\n",
    "    legend = ax.get_legend()\n",
    "    if legend is not None:\n",
    "        legend.remove()\n",
    "\n",
    "    if fname is not None:\n",
    "        fig.savefig(os.path.join(figdir, fname + '.pdf'), bbox_inches='tight')\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figures 2 and S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention(sustained, fname='sustained_attention');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_attention(sustained_unfiltered, fname='sustained_attention');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention(sustained, fname='sustained_attention_by_category', hue='Category');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_attention(sustained_unfiltered, fname='sustained_attention_by_category', hue='Category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention(variable, fname='variable_attention');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention(variable, fname='variable_attention_by_category', hue='Category');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats!\n",
    "\n",
    "Within-condition comparisons by attention level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttests_by_attention_level(df, category=None):\n",
    "    def print_ttest_results(results, label1, label2):\n",
    "        if results.pvalue < 0.001:\n",
    "            p_string = 'p < 0.001'\n",
    "        else:\n",
    "            p_string = f'p = {results.pvalue:.3f}'\n",
    "\n",
    "        print(f'{label1} vs. {label2}: $t({results.df}) = {results.statistic:.3f}, {p_string}$')\n",
    "\n",
    "    if category is not None:\n",
    "        if type(category) is str:\n",
    "            category = [category]\n",
    "        for c in category:\n",
    "            print(f'\\nCategory: {c}')\n",
    "            ttests_by_attention_level(df.query('Category == @c'))\n",
    "\n",
    "        print('\\n\\Within-level tests:')\n",
    "        df = df.groupby(['Subject', 'Attention', 'Category']).mean(numeric_only=True)['Familiarity Rating'].reset_index().pivot(index=['Subject', 'Category'], columns='Attention', values='Familiarity Rating').reset_index().set_index('Subject')\n",
    "\n",
    "        for c in df.columns[1:]:\n",
    "            for i, c1 in enumerate(category):\n",
    "                for c2 in category[i + 1:]:\n",
    "                    print_ttest_results(ttest_rel(df.loc[df['Category'] == c1, c], df.loc[df['Category'] == c2, c]), f'{c} {c1}', c2)\n",
    "\n",
    "        return\n",
    "    \n",
    "    # re-organize dataframe-- rows: subjects; columns: attention levels\n",
    "    df = df.groupby(['Subject', 'Attention']).mean(numeric_only=True)['Familiarity Rating'].reset_index().pivot(index='Subject', columns='Attention', values='Familiarity Rating')\n",
    "\n",
    "    # run t-tests\n",
    "    for i, c1 in enumerate(df.columns):\n",
    "        for c2 in df.columns[i + 1:]:\n",
    "            print_ttest_results(ttest_rel(df[c1], df[c2]), c1, c2)            \n",
    "    \n",
    "    print('\\n')\n",
    "\n",
    "    # old vs. new\n",
    "    print_ttest_results(ttest_rel(df[['Attended', 'Attended category', 'Attended location', 'Unattended']].mean(axis=1), df['Novel']), 'Old', 'New')\n",
    "\n",
    "    # location benefit\n",
    "    print_ttest_results(ttest_rel(df[['Attended', 'Attended location']].mean(axis=1), df['Novel']), 'Attended + Attended location', 'Novel')\n",
    "\n",
    "    # category benefit\n",
    "    print_ttest_results(ttest_rel(df[['Attended', 'Attended category']].mean(axis=1), df['Novel']), 'Attended + Attended category', 'Novel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttests_by_attention_level(sustained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttests_by_attention_level(sustained, category=['Face', 'Place'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttests_by_attention_level(variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttests_by_attention_level(variable, category=['Face', 'Place'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Across-condition comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def across_condition_ttests_by_attention_level(df1, df2, category=None, names=['Sustained', 'Variable']):\n",
    "    def print_ttest_results(a, b, prefix):\n",
    "        results = ttest_ind(a, b)\n",
    "        if results.pvalue < 0.001:\n",
    "            p_string = 'p < 0.001'\n",
    "        else:\n",
    "            p_string = f'p = {results.pvalue:.3f}'\n",
    "\n",
    "        df = len(a) + len(b) - 2\n",
    "        label1 = f'{prefix} -- {names[0]}'\n",
    "        label2 = names[1]\n",
    "        print(f'{label1} vs. {label2}: $t({df}) = {results.statistic:.3f}, {p_string}$')\n",
    "\n",
    "    if category is not None:\n",
    "        if type(category) is str:\n",
    "            category = [category]\n",
    "        for c in category:\n",
    "            print(f'\\nCategory: {c}')\n",
    "            across_condition_ttests_by_attention_level(df1.query('Category == @c'), df2.query('Category == @c'), names=names)\n",
    "        \n",
    "        print('\\nWithin-level tests:')\n",
    "        df1 = df1.groupby(['Subject', 'Attention', 'Category']).mean(numeric_only=True)['Familiarity Rating'].reset_index().pivot(index=['Subject', 'Category'], columns='Attention', values='Familiarity Rating').reset_index().set_index('Subject')\n",
    "        df2 = df2.groupby(['Subject', 'Attention', 'Category']).mean(numeric_only=True)['Familiarity Rating'].reset_index().pivot(index=['Subject', 'Category'], columns='Attention', values='Familiarity Rating').reset_index().set_index('Subject')\n",
    "\n",
    "        for attention in df1.columns[1:]:\n",
    "            for cat in category:\n",
    "                print_ttest_results(df1.loc[df1['Category'] == cat, attention], df2.loc[df2['Category'] == cat, attention], f'{attention} {cat}')\n",
    "\n",
    "        return\n",
    "    \n",
    "    # re-organize dataframe-- rows: subjects; columns: attention levels\n",
    "    df1 = df1.groupby(['Subject', 'Attention']).mean(numeric_only=True)['Familiarity Rating'].reset_index().pivot(index='Subject', columns='Attention', values='Familiarity Rating')\n",
    "    df2 = df2.groupby(['Subject', 'Attention']).mean(numeric_only=True)['Familiarity Rating'].reset_index().pivot(index='Subject', columns='Attention', values='Familiarity Rating')\n",
    "\n",
    "    # run t-tests\n",
    "    for attention in df1.columns:\n",
    "        print_ttest_results(df1[attention], df2[attention], attention)\n",
    "    \n",
    "    print('\\n')\n",
    "\n",
    "    # old\n",
    "    print_ttest_results(df1[['Attended', 'Attended category', 'Attended location', 'Unattended']].mean(axis=1), df2[['Attended', 'Attended category', 'Attended location', 'Unattended']].mean(axis=1), 'Old')\n",
    "\n",
    "    # location benefit\n",
    "    print_ttest_results(df1[['Attended', 'Attended location']].mean(axis=1), df2[['Attended', 'Attended location']].mean(axis=1), 'Attended + Attended location')\n",
    "\n",
    "    # category benefit\n",
    "    print_ttest_results(df1[['Attended', 'Attended category']].mean(axis=1), df2[['Attended', 'Attended category']].mean(axis=1), 'Attended + Attended category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "across_condition_ttests_by_attention_level(sustained, variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "across_condition_ttests_by_attention_level(sustained, variable, category=['Face', 'Place'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Face vs. place differences by condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_vs_place_ttest(df):\n",
    "    df = df.groupby(['Subject', 'Category']).mean(numeric_only=True)['Familiarity Rating'].reset_index().pivot(index='Subject', columns='Category', values='Familiarity Rating')\n",
    "    results = ttest_rel(df['Place'], df['Face'])\n",
    "    if results.pvalue < 0.001:\n",
    "        p_string = 'p < 0.001'\n",
    "    else:\n",
    "        p_string = f'p = {results.pvalue:.3f}'\n",
    "    print(f'$t({results.df}) = {results.statistic:.3f}, {p_string}$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Sustained condition: place vs. face familiarity')\n",
    "face_vs_place_ttest(sustained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Variable condition: place vs. face familiarity')\n",
    "face_vs_place_ttest(variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serial position effects during *encoding*\n",
    "\n",
    "Plot familiarity as a function of presentation position:\n",
    "  - $x$-axis: study position\n",
    "  - $y$-axis: familiarity (at recall)\n",
    "  - color: attention level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_df(df):\n",
    "    df = df.query('`Trial Type` == \"Presentation\"')[['Subject', 'Run', 'Order', 'Attended', 'Attended category', 'Attended location', 'Unattended', 'Cued Location', 'Cued Category']]\n",
    "    df = df.drop('Run', axis=1).groupby(['Subject', 'Order']).mean(numeric_only=True).reset_index()\n",
    "    return df.melt(id_vars=['Subject', 'Order'], value_vars=['Attended', 'Attended category', 'Attended location', 'Unattended'], var_name='Attention', value_name='Familiarity Rating')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 4A and B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(7, 3), sharey=True, sharex=True)\n",
    "sns.lineplot(encoding_df(sustained), x='Order', y='Familiarity Rating', hue='Attention', ax=ax[0], legend=False, palette=attention_colors)\n",
    "ax[0].set_xlabel('Presentation position', fontsize=12)\n",
    "ax[0].set_ylabel('Familiarity rating', fontsize=12)\n",
    "ax[0].set_title('Sustained', fontsize=12)\n",
    "sns.despine(top=True, right=True)\n",
    "\n",
    "sns.lineplot(encoding_df(variable), x='Order', y='Familiarity Rating', hue='Attention', ax=ax[1], legend=False, palette=attention_colors)\n",
    "ax[1].set_xlabel('Presentation position', fontsize=12)\n",
    "ax[1].set_ylabel('')\n",
    "ax[1].set_title('Variable', fontsize=12)\n",
    "sns.despine(top=True, right=True)\n",
    "\n",
    "ax[0].set_xlim([0, 9])\n",
    "ax[0].set_ylim([1, 3.5])\n",
    "plt.tight_layout()\n",
    "\n",
    "fig.savefig(os.path.join(figdir, 'encoding_effects.pdf'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot familiarity as a function of recall position:\n",
    "  - $x$-axis: recall position\n",
    "  - $y$-axis: familiarity\n",
    "  - color: attention level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 4C and D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(7, 3), sharey=True, sharex=True)\n",
    "\n",
    "sns.lineplot(sustained.query('`Trial Type` == \"Memory\"'), x='Order', y='Familiarity Rating', hue='Attention', palette=attention_colors, legend=False, ax=ax[0])\n",
    "ax[0].set_xlabel('Probe position', fontsize=12)\n",
    "ax[0].set_ylabel('Familiarity rating', fontsize=12)\n",
    "ax[0].set_title('Sustained', fontsize=12)\n",
    "sns.despine(top=True, right=True)\n",
    "\n",
    "sns.lineplot(variable.query('`Trial Type` == \"Memory\"'), x='Order', y='Familiarity Rating', hue='Attention', palette=attention_colors, legend=False, ax=ax[1])\n",
    "ax[1].set_xlabel('Probe position', fontsize=12)\n",
    "ax[1].set_ylabel('')\n",
    "ax[1].set_title('Variable', fontsize=12)\n",
    "sns.despine(top=True, right=True)\n",
    "\n",
    "ax[0].set_xlim([0, 39])\n",
    "ax[0].set_ylim([1, 3.5])\n",
    "plt.tight_layout()\n",
    "\n",
    "fig.savefig(os.path.join(figdir, 'familiarity_by_probe_position.pdf'), bbox_inches='tight')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cue_effect_heatmaps(df, fname=None):\n",
    "\n",
    "    attention_levels = ['Attended', 'Attended category', 'Attended location', 'Unattended']\n",
    "\n",
    "    cue_matches = ['Same cue sequence length', 'Category sequence length', 'Location sequence length']\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=len(cue_matches), ncols=len(attention_levels), figsize=(12, 9), sharey=True, sharex='row')\n",
    "    x = df.query('`Trial Type` == \"Presentation\"')\n",
    "\n",
    "    for j, c in enumerate(cue_matches):\n",
    "        for i, a in enumerate(attention_levels):            \n",
    "            sns.histplot(data=x, x=c, y=a, discrete=True, cmap=sns.light_palette(attention_colors[a], as_cmap=True), stat='probability', common_norm=True, cbar=False, ax=ax[j, i], vmin=0, vmax=0.05)\n",
    "            sns.regplot(data=x, x=c, y=a, color='k', scatter=False, ax=ax[j, i])\n",
    "            ax[j, i].set_xlabel(c, fontsize=12)\n",
    "\n",
    "            if j == 0:\n",
    "                ax[j, i].set_title(a, fontsize=14)\n",
    "            if i == 0:\n",
    "                ax[j, i].set_ylabel('Familiarity rating', fontsize=12)\n",
    "            else:\n",
    "                ax[j, i].set_ylabel('')\n",
    "            \n",
    "            ax[j, i].set_yticks([1, 2, 3, 4])\n",
    "            \n",
    "            ax[j, i].set_xlim([0.5, x[c].max() + 0.5])\n",
    "            ax[j, i].set_ylim([0.5, 4.5])\n",
    "\n",
    "            vals = x[[a, c]].dropna(how='any', axis=0)\n",
    "            r = pearsonr(vals[c], vals[a])\n",
    "            if r.pvalue < 0.001:\n",
    "                p_string = 'p < 0.001'\n",
    "            else:\n",
    "                p_string = f'p = {r.pvalue:.3f}'\n",
    "\n",
    "            print(f'{a} {c} $r = {r.statistic:.3f}, {p_string}$')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if fname is not None:\n",
    "        fig.savefig(os.path.join(figdir, fname + '.pdf'), bbox_inches='tight')\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure S4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cue_effect_heatmaps(sustained, fname='sustained_cue_effects');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in attention_colors.items():\n",
    "    plot_colorbar(sns.light_palette(v, as_cmap=True), f'{k.lower()}_light_colorbar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cue_effect_heatmaps(variable, fname='variable_cue_effects');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recency and response biases:\n",
    "  - For each probe, compute the temporal distance (in image presentations) to the nearest same-category cue\n",
    "  - For each probe, compute the number of same-category cues from the current run\n",
    "  - For each probe, compute a recency-weighted average of the number of same-category cues on most recent run, where\n",
    "\n",
    "$w = \\argmax\\left[1 - \\exp\\{-\\frac{x}{\\tau}\\} , \\epsilon \\right]$,\n",
    "\n",
    "and where $w$ is the weight given to the cue at presentation position $x$, $\\tau = 2$, and $\\epsilon = 0.05$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure S5C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(4, 2.8))\n",
    "\n",
    "x = np.linspace(0, 9, 10)\n",
    "plt.plot(x, recency(x, tau=2, eps=0.05) / sum(recency(x, tau=2, eps=0.05)), 'ko-')\n",
    "plt.xlabel('Presentation position', fontsize=12)\n",
    "plt.ylabel('Recency weight', fontsize=12)\n",
    "plt.ylim([0, 0.14])\n",
    "plt.xlim([-0.3, 9.3])\n",
    "sns.despine(top=True, right=True)\n",
    "\n",
    "fig.savefig(os.path.join(figdir, 'recency_weights.pdf'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure S5A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(4, 2.8))\n",
    "# sns.barplot(sustained.query('`Trial Type` == \"Memory\" and Attention == \"Novel\"'), x='Category-matched recent cue', y='Familiarity Rating', color=attention_colors['Novel'])\n",
    "# plt.xlabel('Category-matched recent cue', fontsize=12)\n",
    "# plt.ylabel('Familiarity rating', fontsize=12)\n",
    "# plt.ylim([1.6, 3])\n",
    "# plt.tight_layout()\n",
    "# sns.despine(top=True, right=True)\n",
    "\n",
    "# # statistical test\n",
    "# vals = sustained.query('`Trial Type` == \"Memory\" and Attention == \"Novel\"').groupby(['Subject', 'Category-matched recent cue']).mean(numeric_only=True).reset_index().set_index('Subject')[['Category-matched recent cue', 'Familiarity Rating']].dropna(how='any', axis=0)\n",
    "\n",
    "# fig.savefig(os.path.join(figdir, 'sustained_category_match_recent_cue.pdf'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look for familiarity differences (in the sustained condition) between attended vs. unattended category novel images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = ttest_rel(vals.query('`Category-matched recent cue` == True')['Familiarity Rating'], vals.query('`Category-matched recent cue` == False')['Familiarity Rating'])\n",
    "# if result.pvalue < 0.001:\n",
    "#     p_string = 'p < 0.001'\n",
    "# else:\n",
    "#     p_string = f'p = {result.pvalue:.3f}'\n",
    "# print(f'Response bias (increase in familiarity for novel images from the most recently cued category): $t({result.df}) = {result.statistic:.3f}, {p_string}$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look for familiarity differences (in the sustained condition) between attended category *targets* vs. attended category *lures*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attended_category_targets = sustained.query('`Trial Type` == \"Memory\" and Attention == \"Attended category\"').groupby(['Subject', 'Category-matched recent cue']).mean(numeric_only=True).reset_index().set_index('Subject')[['Familiarity Rating']].dropna(how='any', axis=0)\n",
    "# attended_category_lures = sustained.query('`Trial Type` == \"Memory\" and Attention == \"Novel\" and `Category-matched recent cue`').groupby(['Subject']).mean(numeric_only=True).reset_index().set_index('Subject')[['Familiarity Rating']].dropna(how='any', axis=0)\n",
    "\n",
    "# result = ttest_rel(attended_category_targets['Familiarity Rating'], attended_category_lures['Familiarity Rating'])\n",
    "# if result.pvalue < 0.001:\n",
    "#     p_string = 'p < 0.001'\n",
    "# else:\n",
    "#     p_string = f'p = {result.pvalue:.3f}'\n",
    "# print(f'Familiarity for attended category targets vs lures: $t({result.df}) = {result.statistic:.3f}, {p_string}$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cue_recency_heatmaps(df, metrics=None, fname=None):\n",
    "\n",
    "    attention_levels = ['Attended', 'Attended category', 'Attended location', 'Unattended', 'Novel']\n",
    "\n",
    "    if metrics is None:\n",
    "        metrics = ['Distance to nearest same-category cue', 'Recency-weighted number of same-category cues']\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=len(metrics), ncols=len(attention_levels), figsize=(15, 6), sharey=True, sharex='row')\n",
    "    x = df.query('`Trial Type` == \"Memory\"')\n",
    "\n",
    "    for j, c in enumerate(metrics):\n",
    "        for i, a in enumerate(attention_levels):\n",
    "            xa = x.query('Attention == @a').sort_values(by=[c])\n",
    "\n",
    "            if xa.shape[0] == 0:\n",
    "                ax[j, i].set_axis_off()\n",
    "                continue\n",
    "\n",
    "            if c == 'Recency-weighted number of same-category cues':\n",
    "                bins = np.histogram(xa[c], bins=25)[1]\n",
    "                xa['Digitized recency-weighted number of same-category cues'] = np.digitize(xa[c], bins)\n",
    "                cx = 'Digitized recency-weighted number of same-category cues'\n",
    "                vmax = 0.025\n",
    "            else:\n",
    "                vmax = 0.15\n",
    "                cx = c\n",
    "\n",
    "\n",
    "            sns.histplot(data=xa, x=cx, y='Familiarity Rating', discrete=True, cmap=sns.light_palette(attention_colors[a], as_cmap=True), stat='probability', vmin=0, vmax=vmax, common_norm=True, cbar=False, ax=ax[j, i])\n",
    "            sns.regplot(data=xa, x=cx, y='Familiarity Rating', color='k', scatter=False, ax=ax[j, i])\n",
    "            ax[j, i].set_xlabel(c.replace('same-', '\\nsame-'), fontsize=12)\n",
    "\n",
    "            if j == 0:\n",
    "                ax[j, i].set_title(a, fontsize=14)\n",
    "            if i == 0:\n",
    "                ax[j, i].set_ylabel('Familiarity rating', fontsize=12)\n",
    "            else:\n",
    "                ax[j, i].set_ylabel('')\n",
    "            \n",
    "            ax[j, i].set_yticks([1, 2, 3, 4])\n",
    "\n",
    "            ax[j, i].set_xlim([0.5, xa[cx].max() + 0.5])\n",
    "            ax[j, i].set_ylim([0.5, 4.5])                                                \n",
    "\n",
    "            vals = xa[['Familiarity Rating', c]].dropna(how='any', axis=0)\n",
    "            r = pearsonr(vals[c], vals['Familiarity Rating'])\n",
    "            if r.pvalue < 0.001:\n",
    "                p_string = 'p < 0.001'\n",
    "            else:\n",
    "                p_string = f'p = {r.pvalue:.3f}'\n",
    "\n",
    "            print(f'{a} {c} $r = {r.statistic:.3f}, {p_string}$')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # fix up recency-weighted labels\n",
    "    for j, c in enumerate(metrics):\n",
    "        if c == 'Recency-weighted number of same-category cues':\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")  # ignore FixedLocator warning\n",
    "                ax[j, 0].set_xticklabels([f'{bins[int(i) - 1] + bins[int(i)] / 2:0.2f}' if (i > 1 and i < len(bins)) else '' for i in ax[j, 0].get_xticks()])\n",
    "\n",
    "    if fname is not None:\n",
    "        fig.savefig(os.path.join(figdir, fname + '.pdf'), bbox_inches='tight')\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure S5B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cue_recency_heatmaps(variable, fname='cue_recency_heatmaps_variable');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "3b93b121883a728af7993b526ff0ee0b093293365821a185a02e3143a72feb1b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
